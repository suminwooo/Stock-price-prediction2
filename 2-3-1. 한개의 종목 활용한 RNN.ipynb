{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에서 만든 class를 활용하여 자연어 처리 기법으로 다음으로 나올 class를 예측 해봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/class.csv')\n",
    "train = np.array(data['a5930'][:-20]) # 20개는 테스트 셋으로 활용 예정\n",
    "test = data['a5930'][-20:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 총 길이: 236\n",
      "['c25' 'c23' 'c23' 'c03' 'c25' 'c13' 'c05' 'c13' 'c03' 'c01']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 길이 = 그 안에 있는 class의 수\n",
    "print ('데이터의 총 길이: {}'.format(len(train)))\n",
    "\n",
    "# 데이터의 처음 10개의 class\n",
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 unique class\n",
      "  'c01':   0,\n",
      "  'c03':   1,\n",
      "  'c05':   2,\n",
      "  'c06':   3,\n",
      "  'c08':   4,\n",
      "  'c10':   5,\n",
      "  'c11':   6,\n",
      "  'c13':   7,\n",
      "  'c15':   8,\n",
      "  'c16':   9,\n",
      "  'c18':  10,\n",
      "  'c20':  11,\n",
      "  'c21':  12,\n",
      "  'c23':  13,\n",
      "  'c25':  14,\n",
      "  'NO':  15,\n",
      "index of NO: 15\n"
     ]
    }
   ],
   "source": [
    "# class_set 생성\n",
    "class_set = sorted(set(train)) # 텍스트에 들어간 각 단어가 중복되지 않는 리스트\n",
    "class_set.append('NO') # 리스트에 존재하지 않는 토큰을 나타내는 'NO' 를 첨가\n",
    "print ('{} unique class'.format(len(class_set)))\n",
    "\n",
    "# class_set을 숫자로 맵핑\n",
    "word_idx = {u:i for i, u in enumerate(class_set)}\n",
    "idx_word = np.array(class_set)\n",
    "\n",
    "text_as_int = np.array([word_idx[c] for c in train])\n",
    "\n",
    "# word_idx 확인\n",
    "for word,_ in zip(word_idx, range(len(class_set))):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word_idx[word]))\n",
    "print('index of NO: {}'.format(word_idx['NO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c25' 'c23' 'c23' 'c03' 'c25' 'c13' 'c05' 'c13' 'c03' 'c01']\n",
      "[14 13 13  1 14  7  2  7  1  0]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 확인\n",
    "print(train[:10])\n",
    "print(text_as_int[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c25' 'c23' 'c23' 'c03' 'c25' 'c13' 'c05' 'c13' 'c03' 'c01' 'c20' 'c18'] 12 개\n",
      "[14 13 13  1 14  7  2  7  1  0 11 10] 12 개\n"
     ]
    }
   ],
   "source": [
    "#학습을 위한 데이터 세트 만들기\n",
    "seq_length = 10 # 10개의 단어가 주어졌을 때 다음 단어를 예측하도록 데이터를 만듬\n",
    "examples_per_epoch = len(text_as_int) // seq_length\n",
    "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # dataset를 생성하는 코드\n",
    "\n",
    "sentence_dataset = sentence_dataset.batch(seq_length+2, drop_remainder=True) # +2은 정답이 될 2개의 클래스를 합쳐서 반환하기 위함.\n",
    "for item in sentence_dataset.take(1):\n",
    "    print(idx_word[item.numpy()], len(idx_word[item.numpy()]),'개') # 10개를 활용해 12개가 나옴\n",
    "    print(item.numpy(), len(item.numpy()),'개') # 마찬가지로 12개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c25' 'c23' 'c23' 'c03' 'c25' 'c13' 'c05' 'c13' 'c03' 'c01']\n",
      "[14 13 13  1 14  7  2  7  1  0]\n",
      "c20\n",
      "11\n",
      "['c05' 'c23' 'c13' 'c13' 'c13' 'c03' 'c05' 'c18' 'c03' 'c15']\n",
      "[ 2 13  7  7  7  1  2 10  1  8]\n",
      "c20\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "def split(data): # 2개의 단어를 잘라줌(2개를 예측하였기 때문에)\n",
    "    return [data[:-2], data[-2]]\n",
    "\n",
    "train_dataset = sentence_dataset.map(split) # 새로운 데이터셋 생성\n",
    "for x,y in train_dataset.take(2):\n",
    "    print(idx_word[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx_word[y.numpy()])\n",
    "    print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 #한번에 10개의 데이터를 사용 \n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 20)            320       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 64)            21760     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 35,024\n",
      "Trainable params: 35,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "total_words = len(class_set)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words,20, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=64, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.LSTM(units=32),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 49:\n",
    "        return\n",
    "    test_sentence = data['a28260'][-20:]\n",
    "\n",
    "    next_words = 5\n",
    "    for _ in range(next_words):\n",
    "        test_text_X = test_sentence[-seq_length:]\n",
    "        test_text_X = np.array([word_idx[c] if c in word_idx else word_idx['NO'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word_idx['NO'])\n",
    "\n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += ' ' + idx_word[output_idx[0]]\n",
    "        \n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2 steps\n",
      "Epoch 1/100\n",
      "2/2 - 3s - loss: 2.7709 - accuracy: 0.0500\n",
      "Epoch 2/100\n",
      "2/2 - 0s - loss: 2.7641 - accuracy: 0.3000\n",
      "Epoch 3/100\n",
      "2/2 - 0s - loss: 2.7588 - accuracy: 0.2000\n",
      "Epoch 4/100\n",
      "2/2 - 0s - loss: 2.7525 - accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "2/2 - 0s - loss: 2.7478 - accuracy: 0.2000\n",
      "Epoch 6/100\n",
      "2/2 - 0s - loss: 2.7352 - accuracy: 0.2000\n",
      "Epoch 7/100\n",
      "2/2 - 0s - loss: 2.7275 - accuracy: 0.1500\n",
      "Epoch 8/100\n",
      "2/2 - 0s - loss: 2.7308 - accuracy: 0.1000\n",
      "Epoch 9/100\n",
      "2/2 - 0s - loss: 2.7122 - accuracy: 0.0500\n",
      "Epoch 10/100\n",
      "2/2 - 0s - loss: 2.6744 - accuracy: 0.2500\n",
      "Epoch 11/100\n",
      "2/2 - 0s - loss: 2.6977 - accuracy: 0.1500\n",
      "Epoch 12/100\n",
      "2/2 - 0s - loss: 2.6068 - accuracy: 0.2000\n",
      "Epoch 13/100\n",
      "2/2 - 0s - loss: 2.6167 - accuracy: 0.2000\n",
      "Epoch 14/100\n",
      "2/2 - 0s - loss: 2.6302 - accuracy: 0.1500\n",
      "Epoch 15/100\n",
      "2/2 - 0s - loss: 2.5472 - accuracy: 0.1500\n",
      "Epoch 16/100\n",
      "2/2 - 0s - loss: 2.5821 - accuracy: 0.1500\n",
      "Epoch 17/100\n",
      "2/2 - 0s - loss: 2.3817 - accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "2/2 - 0s - loss: 2.3359 - accuracy: 0.2000\n",
      "Epoch 19/100\n",
      "2/2 - 0s - loss: 2.5996 - accuracy: 0.1000\n",
      "Epoch 20/100\n",
      "2/2 - 0s - loss: 2.5659 - accuracy: 0.0500\n",
      "Epoch 21/100\n",
      "2/2 - 0s - loss: 2.4949 - accuracy: 0.1500\n",
      "Epoch 22/100\n",
      "2/2 - 0s - loss: 2.3318 - accuracy: 0.1500\n",
      "Epoch 23/100\n",
      "2/2 - 0s - loss: 2.2610 - accuracy: 0.0500\n",
      "Epoch 24/100\n",
      "2/2 - 0s - loss: 2.3782 - accuracy: 0.1000\n",
      "Epoch 25/100\n",
      "2/2 - 0s - loss: 2.3091 - accuracy: 0.2000\n",
      "Epoch 26/100\n",
      "2/2 - 0s - loss: 2.4771 - accuracy: 0.1500\n",
      "Epoch 27/100\n",
      "2/2 - 0s - loss: 2.4495 - accuracy: 0.1500\n",
      "Epoch 28/100\n",
      "2/2 - 0s - loss: 2.3367 - accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "2/2 - 0s - loss: 2.1900 - accuracy: 0.2500\n",
      "Epoch 30/100\n",
      "2/2 - 0s - loss: 2.3543 - accuracy: 0.2500\n",
      "Epoch 31/100\n",
      "2/2 - 0s - loss: 2.2738 - accuracy: 0.2000\n",
      "Epoch 32/100\n",
      "2/2 - 0s - loss: 2.2694 - accuracy: 0.2500\n",
      "Epoch 33/100\n",
      "2/2 - 0s - loss: 2.1606 - accuracy: 0.3000\n",
      "Epoch 34/100\n",
      "2/2 - 0s - loss: 2.2855 - accuracy: 0.1500\n",
      "Epoch 35/100\n",
      "2/2 - 0s - loss: 2.2110 - accuracy: 0.1500\n",
      "Epoch 36/100\n",
      "2/2 - 0s - loss: 2.1999 - accuracy: 0.2000\n",
      "Epoch 37/100\n",
      "2/2 - 0s - loss: 2.1569 - accuracy: 0.2500\n",
      "Epoch 38/100\n",
      "2/2 - 0s - loss: 2.0542 - accuracy: 0.4500\n",
      "Epoch 39/100\n",
      "2/2 - 0s - loss: 1.9434 - accuracy: 0.4000\n",
      "Epoch 40/100\n",
      "2/2 - 0s - loss: 1.8684 - accuracy: 0.4500\n",
      "Epoch 41/100\n",
      "2/2 - 0s - loss: 1.9038 - accuracy: 0.4000\n",
      "Epoch 42/100\n",
      "2/2 - 0s - loss: 1.9609 - accuracy: 0.3500\n",
      "Epoch 43/100\n",
      "2/2 - 0s - loss: 1.8031 - accuracy: 0.4500\n",
      "Epoch 44/100\n",
      "2/2 - 0s - loss: 1.9197 - accuracy: 0.3500\n",
      "Epoch 45/100\n",
      "2/2 - 0s - loss: 1.7164 - accuracy: 0.4500\n",
      "Epoch 46/100\n",
      "2/2 - 0s - loss: 1.8002 - accuracy: 0.4000\n",
      "Epoch 47/100\n",
      "2/2 - 0s - loss: 1.5826 - accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "2/2 - 0s - loss: 1.6820 - accuracy: 0.4500\n",
      "Epoch 49/100\n",
      "2/2 - 0s - loss: 1.5117 - accuracy: 0.6000\n",
      "Epoch 50/100\n",
      "2/2 - 0s - loss: 1.8549 - accuracy: 0.3500\n",
      "Epoch 51/100\n",
      "2/2 - 0s - loss: 1.6103 - accuracy: 0.4500\n",
      "Epoch 52/100\n",
      "2/2 - 0s - loss: 1.6506 - accuracy: 0.4500\n",
      "Epoch 53/100\n",
      "2/2 - 0s - loss: 1.6567 - accuracy: 0.3500\n",
      "Epoch 54/100\n",
      "2/2 - 0s - loss: 1.5356 - accuracy: 0.5500\n",
      "Epoch 55/100\n",
      "2/2 - 0s - loss: 1.5338 - accuracy: 0.4500\n",
      "Epoch 56/100\n",
      "2/2 - 0s - loss: 1.2803 - accuracy: 0.6000\n",
      "Epoch 57/100\n",
      "2/2 - 0s - loss: 1.3435 - accuracy: 0.4500\n",
      "Epoch 58/100\n",
      "2/2 - 0s - loss: 1.0688 - accuracy: 0.6500\n",
      "Epoch 59/100\n",
      "2/2 - 0s - loss: 1.3909 - accuracy: 0.4500\n",
      "Epoch 60/100\n",
      "2/2 - 0s - loss: 1.1724 - accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "2/2 - 0s - loss: 1.3199 - accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "2/2 - 0s - loss: 1.2294 - accuracy: 0.6000\n",
      "Epoch 63/100\n",
      "2/2 - 0s - loss: 1.1703 - accuracy: 0.6500\n",
      "Epoch 64/100\n",
      "2/2 - 0s - loss: 1.0343 - accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "2/2 - 0s - loss: 1.3364 - accuracy: 0.4500\n",
      "Epoch 66/100\n",
      "2/2 - 0s - loss: 1.0888 - accuracy: 0.6000\n",
      "Epoch 67/100\n",
      "2/2 - 0s - loss: 1.0582 - accuracy: 0.6500\n",
      "Epoch 68/100\n",
      "2/2 - 0s - loss: 1.1324 - accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "2/2 - 0s - loss: 1.1203 - accuracy: 0.6000\n",
      "Epoch 70/100\n",
      "2/2 - 0s - loss: 0.9587 - accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "2/2 - 0s - loss: 1.0533 - accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "2/2 - 0s - loss: 0.8523 - accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "2/2 - 0s - loss: 0.8511 - accuracy: 0.8500\n",
      "Epoch 74/100\n",
      "2/2 - 0s - loss: 0.7514 - accuracy: 0.8500\n",
      "Epoch 75/100\n",
      "2/2 - 0s - loss: 0.9078 - accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "2/2 - 0s - loss: 0.6961 - accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "2/2 - 0s - loss: 0.6502 - accuracy: 0.9000\n",
      "Epoch 78/100\n",
      "2/2 - 0s - loss: 0.6574 - accuracy: 0.9000\n",
      "Epoch 79/100\n",
      "2/2 - 0s - loss: 0.7717 - accuracy: 0.7500\n",
      "Epoch 80/100\n",
      "2/2 - 0s - loss: 0.6876 - accuracy: 0.9000\n",
      "Epoch 81/100\n",
      "2/2 - 0s - loss: 0.6100 - accuracy: 0.8500\n",
      "Epoch 82/100\n",
      "2/2 - 0s - loss: 0.7484 - accuracy: 0.8500\n",
      "Epoch 83/100\n",
      "2/2 - 0s - loss: 0.6715 - accuracy: 0.8500\n",
      "Epoch 84/100\n",
      "2/2 - 0s - loss: 0.5962 - accuracy: 0.8500\n",
      "Epoch 85/100\n",
      "2/2 - 0s - loss: 0.6549 - accuracy: 0.8500\n",
      "Epoch 86/100\n",
      "2/2 - 0s - loss: 0.6441 - accuracy: 0.9000\n",
      "Epoch 87/100\n",
      "2/2 - 0s - loss: 0.5351 - accuracy: 0.9500\n",
      "Epoch 88/100\n",
      "2/2 - 0s - loss: 0.6037 - accuracy: 0.8000\n",
      "Epoch 89/100\n",
      "2/2 - 0s - loss: 0.4667 - accuracy: 0.9500\n",
      "Epoch 90/100\n",
      "2/2 - 0s - loss: 0.4276 - accuracy: 0.9500\n",
      "Epoch 91/100\n",
      "2/2 - 0s - loss: 0.6479 - accuracy: 0.9000\n",
      "Epoch 92/100\n",
      "2/2 - 0s - loss: 0.3974 - accuracy: 0.9500\n",
      "Epoch 93/100\n",
      "2/2 - 0s - loss: 0.5043 - accuracy: 0.9000\n",
      "Epoch 94/100\n",
      "2/2 - 0s - loss: 0.4709 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "2/2 - 0s - loss: 0.5028 - accuracy: 0.8500\n",
      "Epoch 96/100\n",
      "2/2 - 0s - loss: 0.6859 - accuracy: 0.9000\n",
      "Epoch 97/100\n",
      "2/2 - 0s - loss: 0.3692 - accuracy: 0.9500\n",
      "Epoch 98/100\n",
      "2/2 - 0s - loss: 0.6354 - accuracy: 0.9000\n",
      "Epoch 99/100\n",
      "2/2 - 0s - loss: 0.5457 - accuracy: 0.9000\n",
      "Epoch 100/100\n",
      "2/2 - 0s - loss: 0.4687 - accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset.repeat(), epochs=100, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c13 c03 c08 c23 c23 c25 c15 c18 c03 c18 c23 c03 c21 c01 c01 c23 c01 c23 c16 c08\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "test_sentence = 'c13 c03 c08 c23 c23 c25 c15 c18 c03 c18 c23 c03 c21 c01 c01 c23 c01 c23'\n",
    "\n",
    "next_words = 2\n",
    "for _ in range(next_words):\n",
    "    test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "    test_text_X = np.array([word_idx[c] if c in word_idx else word_idx['NO'] for c in test_text_X])\n",
    "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word_idx['NO'])\n",
    "    \n",
    "    output_idx = model.predict_classes(test_text_X)\n",
    "    test_sentence += ' ' + idx_word[output_idx[0]]\n",
    "\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c13', 'c03', 'c08', 'c23', 'c23', 'c25', 'c15', 'c18', 'c03',\n",
       "       'c18', 'c23', 'c03', 'c21', 'c01', 'c01', 'c23', 'c01', 'c23',\n",
       "       'c06', 'c06'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data['a5930'][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
